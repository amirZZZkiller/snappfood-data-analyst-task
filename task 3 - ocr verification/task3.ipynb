{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93300a1c",
   "metadata": {},
   "source": [
    "<h1 style=\"color:#FF00A6; font-weight:bold; font-family:sans-serif;\">\n",
    "  Snappfood Data Analyst Task\n",
    "</h1>\n",
    "\n",
    "<h3 style=\"color:#F9FAFB; font-family:sans-serif;\">\n",
    "  Task 3 – <em>OCR Verification</em>\n",
    "</h3>\n",
    "\n",
    "<p style=\"color:#EBEDF0; font-size:14px; font-family:sans-serif;\">\n",
    "  Extract restaurant names and contact numbers from storefront images using OCR, and verify consistency with structured records.\n",
    "</p>\n",
    "<br>\n",
    "<hr style=\"color:#FF00A6;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ae7c17",
   "metadata": {},
   "source": [
    "# Imports & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d12eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\amirreza\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.5.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\amirreza\\appdata\\roaming\\python\\python311\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\amirreza\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\amirreza\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (1.24.4)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\amirreza\\appdata\\roaming\\python\\python311\\site-packages (from python-dateutil>=2.8.1->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: opencv-python in c:\\users\\amirreza\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.11.0.86)\n",
      "Requirement already satisfied: numpy>=1.21.2 in c:\\users\\amirreza\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from opencv-python) (1.24.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Import the importlib module to check if the required libraries are installed\n",
    "import importlib.util\n",
    "\n",
    "# List of required libraries\n",
    "required_libraries = ['requests', 'pandas', 'tqdm', 'easyocr', 'opencv-python']\n",
    "\n",
    "# Install the required libraries if they are not already installed\n",
    "for lib in required_libraries:\n",
    "    if importlib.util.find_spec(lib) is None:\n",
    "        %pip install {lib}\n",
    "\n",
    "import requests\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import easyocr\n",
    "from tqdm import tqdm\n",
    "from difflib import SequenceMatcher\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1545896",
   "metadata": {},
   "source": [
    "# Prepare Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6749879",
   "metadata": {},
   "source": [
    "## Downlaod from Google Sheets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c519a94d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sheet downloaded and saved to './task3_dataset.csv'\n"
     ]
    }
   ],
   "source": [
    "def download_sheet_as_csv(spreadsheet_id, gid, output_path):\n",
    "    url = f\"https://docs.google.com/spreadsheets/d/{spreadsheet_id}/export\"\n",
    "    params = {\n",
    "        \"format\": \"csv\",\n",
    "        \"gid\": gid\n",
    "    }\n",
    "    response = requests.get(url, params=params)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        with open(output_path, \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "        print(f\"Sheet downloaded and saved to '{output_path}'\")\n",
    "    else:\n",
    "        print(f\"Failed to download sheet. Status code: {response.status_code}\")\n",
    "\n",
    "# Replace with your actual spreadsheet ID and sheet GID\n",
    "SPREADSHEET_ID = \"1ic4RLD_r4ASfl7nRk2ctagH_98j2sPDYN7IB4n6n9e8\"\n",
    "GID = \"912424133\"  # GID of Task 3 sheet\n",
    "OUTPUT_FILE = \"./task3_dataset.csv\"\n",
    "\n",
    "download_sheet_as_csv(SPREADSHEET_ID, GID, OUTPUT_FILE)\n",
    "\n",
    "# Correct the names and save the dataset again\n",
    "df = pd.read_csv(\"./task3_dataset.csv\", encoding=\"utf-8-sig\")\n",
    "df.to_csv(\"task3_dataset.csv\", index=False, encoding=\"utf-8-sig\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764b0337",
   "metadata": {},
   "source": [
    "## Image Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "408f0db8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [01:18<00:00,  7.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All images processed and stored in 'images/'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Config\n",
    "DATA_PATH = \"task3_dataset.csv\"\n",
    "IMAGE_DIR = \"images\"\n",
    "os.makedirs(IMAGE_DIR, exist_ok=True)\n",
    "\n",
    "# Load Dataset\n",
    "df = pd.read_csv(DATA_PATH, encoding=\"utf-8-sig\")\n",
    "\n",
    "# Utility: Extract File ID from Google Drive Link\n",
    "def extract_drive_file_id(link):\n",
    "    match = re.search(r\"/d/([a-zA-Z0-9_-]+)\", link)\n",
    "    return match.group(1) if match else None\n",
    "\n",
    "# Download Function\n",
    "def download_drive_image(file_id, output_path):\n",
    "    \"\"\"Downloads a file from Google Drive using file ID.\"\"\"\n",
    "    URL = \"https://drive.google.com/uc?export=download\"\n",
    "\n",
    "    session = requests.Session()\n",
    "    response = session.get(URL, params={\"id\": file_id}, stream=True)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        with open(output_path, \"wb\") as f:\n",
    "            for chunk in response.iter_content(1024):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "# Process and Download All Images\n",
    "image_paths = []\n",
    "\n",
    "print(\"Downloading images...\")\n",
    "\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    link = row['Photo']\n",
    "    file_id = extract_drive_file_id(link)\n",
    "\n",
    "    if file_id:\n",
    "        filename = f\"{idx}.jpg\"\n",
    "        save_path = os.path.join(IMAGE_DIR, filename)\n",
    "\n",
    "        success = download_drive_image(file_id, save_path)\n",
    "        image_paths.append(save_path if success else None)\n",
    "    else:\n",
    "        image_paths.append(None)\n",
    "\n",
    "# Add Image Path Column to DataFrame\n",
    "df[\"Image_Path\"] = image_paths\n",
    "print(\"All images processed and stored in 'images/'\")\n",
    "\n",
    "# Optional: Save updated dataset with image paths\n",
    "df.to_csv(\"task3_dataset_with_image_paths.csv\", index=False, encoding=\"utf-8-sig\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a4ce59",
   "metadata": {},
   "source": [
    "## Text Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13c71d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text normalization\n",
    "def normalize_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = str(text)\n",
    "    replacements = {\n",
    "        \"ي\": \"ی\", \"ك\": \"ک\", \"ئ\": \"ی\", \"أ\": \"ا\", \"إ\": \"ا\", \"ۀ\": \"ه\", \"ؤ\": \"و\",\n",
    "    }\n",
    "    for src, dst in replacements.items():\n",
    "        text = text.replace(src, dst)\n",
    "    text = re.sub(r\"[\\u064B-\\u0652]\", \"\", text)\n",
    "    text = re.sub(r\"[^\\w\\s\\u0600-\\u06FF]\", \" \", text)\n",
    "    return re.sub(r\"\\s+\", \" \", text).strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760255fd",
   "metadata": {},
   "source": [
    "## Number Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "255c0816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number normalization\n",
    "def normalize_numbers(text):\n",
    "    if pd.isna(text):\n",
    "        return []\n",
    "    text = str(text)\n",
    "    text = text.translate(str.maketrans(\"۰۱۲۳۴۵۶۷۸۹\", \"0123456789\"))\n",
    "    digits = re.findall(r\"\\d{6,}\", text)\n",
    "    return list(set(digits))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb7ae9d",
   "metadata": {},
   "source": [
    "## Apply Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c28b778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize name column\n",
    "df[\"Normalized_Name\"] = df[\"Name\"].apply(normalize_text)\n",
    "\n",
    "# Normalize and split numbers\n",
    "df[\"Normalized_Numbers\"] = df[\"Number\"].apply(normalize_numbers)\n",
    "\n",
    "# Preview\n",
    "df[[\"Name\", \"Normalized_Name\", \"Number\", \"Normalized_Numbers\"]]\n",
    "\n",
    "# Update dataset csv file\n",
    "df.to_csv(\"task3_dataset_with_image_paths.csv\", index=False, encoding=\"utf-8-sig\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf19f1d",
   "metadata": {},
   "source": [
    "# OCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3a35f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]c:\\Users\\Amirreza\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "100%|██████████| 10/10 [01:36<00:00,  9.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OCR pipeline complete using EasyOCR.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "df = pd.read_csv(\"task3_dataset_with_image_paths.csv\", encoding=\"utf-8-sig\")\n",
    "\n",
    "# OCR preprocessing (CLAHE + Threshold)\n",
    "def preprocess_image(path, max_width=1024):\n",
    "    img = cv2.imread(path)\n",
    "    if img is None:\n",
    "        return None\n",
    "    h, w = img.shape[:2]\n",
    "    if w > max_width:\n",
    "        img = cv2.resize(img, (max_width, int(h * max_width / w)))\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0).apply(gray)\n",
    "    th = cv2.adaptiveThreshold(clahe, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
    "                               cv2.THRESH_BINARY, 15, 10)\n",
    "    return th\n",
    "\n",
    "# Run OCR\n",
    "reader = easyocr.Reader(['fa', 'en'], gpu=True)\n",
    "\n",
    "def extract_easyocr_lines(image_path):\n",
    "    img = preprocess_image(image_path)\n",
    "    if img is None:\n",
    "        return []\n",
    "    results = reader.readtext(img, detail=0)\n",
    "    return [line.strip() for line in results if line.strip()]\n",
    "\n",
    "# Post-OCR extraction\n",
    "\n",
    "def extract_best_name(lines):\n",
    "    clean_lines = []\n",
    "    for line in lines:\n",
    "        line = normalize_text(line)\n",
    "        if not line or len(line) < 5:\n",
    "            continue\n",
    "        if re.search(r\"\\d\", line):  # skip digits\n",
    "            continue\n",
    "        if re.search(r\"[\\u0600-\\u06FF]\", line):\n",
    "            clean_lines.append(line)\n",
    "\n",
    "    if not clean_lines:\n",
    "        return \"\"\n",
    "    return sorted(clean_lines, key=lambda x: len(x.split()), reverse=True)[0]\n",
    "\n",
    "def extract_clean_numbers(lines):\n",
    "    all_text = \"\\n\".join(lines).translate(str.maketrans(\"۰۱۲۳۴۵۶۷۸۹\", \"0123456789\"))\n",
    "    raw_numbers = re.findall(r\"\\d{6,}\", all_text)\n",
    "\n",
    "    # Accept numbers starting with known patterns\n",
    "    clean = []\n",
    "    for num in raw_numbers:\n",
    "        if len(num) >= 10 and (num.startswith(\"09\") or num.startswith(\"021\")):\n",
    "            clean.append(num)\n",
    "    return list(set(clean))\n",
    "\n",
    "# Matching & Scoring\n",
    "ocr_names = []\n",
    "ocr_numbers = []\n",
    "name_scores = []\n",
    "name_matches = []\n",
    "number_matches = []\n",
    "\n",
    "for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    img_path = row[\"Image_Path\"]\n",
    "    if not os.path.exists(img_path):\n",
    "        ocr_names.append(\"\")\n",
    "        ocr_numbers.append([])\n",
    "        name_scores.append(0)\n",
    "        name_matches.append(False)\n",
    "        number_matches.append(0)\n",
    "        continue\n",
    "\n",
    "    lines = extract_easyocr_lines(img_path)\n",
    "    name = extract_best_name(lines)\n",
    "    numbers = extract_clean_numbers(lines)\n",
    "\n",
    "    # Fuzzy match\n",
    "    expected_name = normalize_text(row[\"Name\"])\n",
    "    score = int(SequenceMatcher(None, name, expected_name).ratio() * 100)\n",
    "    match = score >= 85\n",
    "\n",
    "    # Phone number match\n",
    "    expected_numbers = normalize_numbers(row[\"Number\"])\n",
    "    matched_numbers = [n for n in numbers if n in expected_numbers]\n",
    "\n",
    "    # Store\n",
    "    ocr_names.append(name)\n",
    "    ocr_numbers.append(numbers)\n",
    "    name_scores.append(score)\n",
    "    name_matches.append(match)\n",
    "    number_matches.append(len(matched_numbers))\n",
    "\n",
    "# Finalize dataframe\n",
    "df[\"OCR_Name\"] = ocr_names\n",
    "df[\"OCR_Numbers\"] = ocr_numbers\n",
    "df[\"Name_Score\"] = name_scores\n",
    "df[\"Name_Match\"] = name_matches\n",
    "df[\"Number_Match_Count\"] = number_matches\n",
    "\n",
    "# Save it\n",
    "df.to_csv(\"task3_output_easyocr.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "print(\"OCR pipeline complete using EasyOCR.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
